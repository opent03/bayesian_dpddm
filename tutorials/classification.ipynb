{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian D-PDDM Tutorial\n",
    "\n",
    "In this tutorial, we will use Bayesian D-PDDM to monitor deteriorating shifts in the UCI Heart Disease dataset. The preprocessed dataset is available in ``data/uci_data/``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x77793fb2a5b0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('../')\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "SEED = 9927\n",
    "np.random.seed(SEED)\n",
    "torch.random.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data\n",
    "\n",
    "We begin with importing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.data_utils import UCIDataset\n",
    "data_dict = torch.load('data/uci_data/uci_heart_torch.pt')\n",
    "uci_dict = {}\n",
    "for k, data in data_dict.items():\n",
    "    data = list(zip(*data))\n",
    "    X, y = torch.stack(data[0]), torch.tensor(data[1], dtype=torch.int)\n",
    "    \n",
    "    # normalize\n",
    "    min_ = torch.min(X, dim=0).values\n",
    "    max_ = torch.max(X, dim=0).values\n",
    "    X = (X - min_) / (max_ - min_)\n",
    "    uci_dict[k] = UCIDataset(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use ``train`` to train the base model, ``valid`` to validate the base model. Then, ``valid`` will be used to train the distribution of i.i.d. disagreement rates Phi.\n",
    "\n",
    "We will monitor on both ``train`` and ``iid_test`` in order to validate that our monitor is well-calibrated, i.e. it resists flagging in-distribution unseen samples. \n",
    "\n",
    "Finally, we monitor ``ood_test`` to assert that our monitor detects deteriorating changes from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict = {}\n",
    "dataset_dict['train'] = uci_dict['train']\n",
    "dataset_dict['valid'] = uci_dict['val']\n",
    "dataset_dict['dpddm_train'] = uci_dict['val']\n",
    "dataset_dict['dpddm_id'] = uci_dict['iid_test']\n",
    "dataset_dict['dpddm_ood'] = uci_dict['ood_test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Bayesian D-PDDM components\n",
    "\n",
    "Bayesian D-PDDM involves two primary components: \n",
    "    \n",
    "- the base model\n",
    "- the monitor\n",
    "\n",
    "The base model will depend on the data modality. For tabular data, we work with ``MLPModel``.  For images, we work with ``ConvModel``.\n",
    "The monitor takes in a base model, training and validation datasets, and a configuration file. We parse these using ``hydra-core``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hydra\n",
    "from omegaconf import DictConfig\n",
    "from bayesian_dpddm import MLPModel, DPDDMBayesianMonitor\n",
    "from experiments.utils import get_configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hydra.initialize(config_path='../experiments/configs', version_base='1.2')\n",
    "args = hydra.compose(config_name=\"uci\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config, train_config = get_configs(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "base_model = MLPModel(model_config, train_size=len(dataset_dict['train']))\n",
    "monitor = DPDDMBayesianMonitor(\n",
    "        model=base_model,\n",
    "        trainset=dataset_dict['train'],\n",
    "        valset=dataset_dict['valid'],\n",
    "        train_cfg=train_config,\n",
    "        device=device,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Classifier Training\n",
    "\n",
    "We are now ready to train the model per our configurations. Simply run ``monitor.train_model``. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/50 [00:00<00:13,  3.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0, train loss: 240.5318\n",
      "Epoch:  0, valid loss: 2.1850\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 11/50 [00:01<00:04,  7.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, train loss: 214.5692\n",
      "Epoch: 10, valid loss: 1.3339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 21/50 [00:02<00:03,  8.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20, train loss: 201.3016\n",
      "Epoch: 20, valid loss: 1.1879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 31/50 [00:03<00:02,  9.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 30, train loss: 192.7989\n",
      "Epoch: 30, valid loss: 1.0537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 41/50 [00:05<00:01,  8.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 40, train loss: 186.4880\n",
      "Epoch: 40, valid loss: 0.9390\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:05<00:00,  8.34it/s]\n"
     ]
    }
   ],
   "source": [
    "output_metrics = monitor.train_model(tqdm_enabled=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training of the maximum i.i.d. disagreement rate distribution\n",
    "\n",
    "We now have a base Bayesian model fitted to the training data. We now seek the disagreement rate with respect to our base classifier of models that:\n",
    "\n",
    "- agree with our base classifier on the training data\n",
    "- disagree with our base classifier on unseen i.i.d. data\n",
    "\n",
    "In order to achieve this, we batch sample from our belief over the decision surface, and repeatedly select the decision surface with the strongest disagreement rate. We collect these disagreement rates into model.Phi.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:02<00:00, 189.10it/s]\n"
     ]
    }
   ],
   "source": [
    "monitor.pretrain_disagreement_distribution(dataset=dataset_dict['dpddm_train'],\n",
    "                                           n_post_samples=args.dpddm.n_post_samples,\n",
    "                                           data_sample_size=args.dpddm.data_sample_size,\n",
    "                                           Phi_size=args.dpddm.Phi_size, \n",
    "                                           temperature=args.dpddm.temp,\n",
    "                                           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute FPRs and TPRs\n",
    "\n",
    "We are essentially done. Our monitor has the essential components:\n",
    "\n",
    "- a trained base classifier on i.i.d. data\n",
    "- a trained distribution of i.i.d. disagreement rates\n",
    "\n",
    "This base classifier is now ready to be deployed on any deployment data, as long as we monitor periodically by running either ``monitor.dpddm_test`` or ``monitor.repeat_tests`` (for repeated testing, useful to compute statistics) on future data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 188.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dpddm_train: 0.04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 188.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dpddm_id: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 189.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dpddm_ood: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "stats = {}\n",
    "dis_rates = {}\n",
    "\n",
    "for k,dataset in {\n",
    "    'dpddm_train': dataset_dict['dpddm_train'],\n",
    "    'dpddm_id': dataset_dict['dpddm_id'],\n",
    "    'dpddm_ood': dataset_dict['dpddm_ood']\n",
    "}.items():\n",
    "    rate, max_dis_rates = monitor.repeat_tests(n_repeats=args.dpddm.n_repeats,\n",
    "                                    dataset=dataset, \n",
    "                                    n_post_samples=args.dpddm.n_post_samples,\n",
    "                                    data_sample_size=args.dpddm.data_sample_size,\n",
    "                                    temperature=args.dpddm.temp\n",
    "                                    )\n",
    "    print(f\"{k}: {rate}\")\n",
    "    stats[k] = rate\n",
    "    dis_rates[k] = (np.mean(max_dis_rates), np.std(max_dis_rates))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above, we notice that the TPR for OOD samples is 1.0, i.e. our bayesian D-PDDM monitor is able to correctly identify deteriorating changes in the data distribution.\n",
    "\n",
    "We further notice that for a held-out in-distribution sample, the model does not identify the sample as out-of-distribution. Indeed, the base classifier achieves similar performance on this held-out set than on the validation set, proving that bayesian D-PDDM incurs low false positive rates for in-distribution samples."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dpddm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
